{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8471e4-1f8a-40ce-a22f-6323aa614e58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 7a0f5bd] Auto update (nb+data) - 2025-12-03 07:55:05\n",
      " 3 files changed, 2819 insertions(+), 2598 deletions(-)\n",
      "Current branch main is up to date.\n",
      "branch 'main' set up to track 'origin/main'.\n",
      "Pushed: PredictNHL.ipynb, enhanced_game_data.csv, predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import hashlib, shutil, subprocess, os\n",
    "from datetime import datetime\n",
    "\n",
    "REPO   = Path(r\"C:\\Users\\Nic\\Desktop\\NHLPredictor\")\n",
    "DESK   = Path(r\"C:\\Users\\Nic\\Desktop\")\n",
    "NBNAME = \"PredictNHL.ipynb\"\n",
    "EXTS   = {\".csv\", \".json\", \".xlsx\"}\n",
    "BRANCH = \"main\"\n",
    "\n",
    "# --- Force a CSV to \"update\" even if content is the same ---\n",
    "FORCE_REFRESH_CSV = True\n",
    "CSV_WHITELIST = []  # e.g. [\"predictions.csv\", \"enhanced_game_data.csv\"]; empty = all CSVs in repo\n",
    "\n",
    "def sha(p):\n",
    "    import hashlib\n",
    "    h=hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for b in iter(lambda:f.read(1<<20), b\"\"): h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def bump_csv_bytes(path: Path):\n",
    "    \"\"\"Toggle trailing newline to force a harmless byte change.\"\"\"\n",
    "    b = path.read_bytes()\n",
    "    if b.endswith(b\"\\n\"):\n",
    "        path.write_bytes(b.rstrip(b\"\\n\"))      # remove last newline\n",
    "    else:\n",
    "        path.write_bytes(b + b\"\\n\")            # add last newline\n",
    "\n",
    "def run(cmd):\n",
    "    r = subprocess.run(cmd, cwd=str(REPO), capture_output=True, text=True)\n",
    "    if r.stdout: print(r.stdout.strip())\n",
    "    if r.stderr and r.returncode != 0: print(r.stderr.strip())\n",
    "    return r.returncode\n",
    "\n",
    "assert REPO.exists()\n",
    "os.chdir(REPO)\n",
    "\n",
    "# 1) Build targets: notebook + tracked csv/json/xlsx\n",
    "targets = {NBNAME}\n",
    "targets |= {p.name for p in REPO.iterdir() if p.is_file() and p.suffix.lower() in EXTS}\n",
    "\n",
    "# 2) Copy Desktop -> repo when content differs\n",
    "changed = []\n",
    "for name in sorted(targets):\n",
    "    src, dst = DESK/name, REPO/name\n",
    "    if src.exists():\n",
    "        if (not dst.exists()) or sha(src) != sha(dst):\n",
    "            shutil.copy2(src, dst); changed.append(name)\n",
    "\n",
    "# 3) Optionally force-refresh CSVs (toggle EOF newline)\n",
    "if FORCE_REFRESH_CSV:\n",
    "    csvs = [p for p in REPO.iterdir() if p.is_file() and p.suffix.lower()==\".csv\"]\n",
    "    if CSV_WHITELIST:\n",
    "        csvs = [p for p in csvs if p.name in CSV_WHITELIST]\n",
    "    for p in csvs:\n",
    "        # only bump if not already modified by step 2 (to avoid double-noise)\n",
    "        if p.name not in changed:\n",
    "            bump_csv_bytes(p); changed.append(p.name)\n",
    "\n",
    "# 4) Commit & push (only if something actually changed)\n",
    "if changed:\n",
    "    run([\"git\",\"add\"] + changed)\n",
    "    msg = f\"Auto update (nb+data) - {datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "    run([\"git\",\"commit\",\"-m\", msg])\n",
    "    run([\"git\",\"pull\",\"--rebase\",\"origin\", BRANCH])\n",
    "    run([\"git\",\"push\",\"-u\",\"origin\", BRANCH])\n",
    "    print(\"Pushed:\", \", \".join(changed))\n",
    "else:\n",
    "    print(\"Nothing to update. (No content changes and force-refresh off or nothing matched.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0af2c54-99e1-49ea-a78d-1509524b2587",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/15.6 MB 8.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/15.6 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.6 MB 8.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.6/15.6 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.0/15.6 MB 9.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.6 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.6 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 9.4 MB/s  0:00:01\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from xgboost) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (1.41.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (5.29.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.22.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.19.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (0.13.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from imbalanced-learn) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from imbalanced-learn) (1.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: shap in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from shap) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from statsmodels) (2.0.2)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from statsmodels) (1.14.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from statsmodels) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\nic\\desktop\\pythondrop\\lib\\site-packages (from lightgbm) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install xgboost\n",
    "!pip install streamlit\n",
    "!pip install imbalanced-learn\n",
    "!pip install shap\n",
    "!pip install statsmodels\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e8701d-ae59-47be-acc0-320e91c564b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Standard library\n",
    "# ========================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import re\n",
    "import unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "\n",
    "# ========================\n",
    "# Data & numerical\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========================\n",
    "# Web scraping\n",
    "# ========================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# ========================\n",
    "# Visualization\n",
    "# ========================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ========================\n",
    "# Statistics\n",
    "# ========================\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import loguniform, uniform\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ========================\n",
    "# Scikit-learn\n",
    "# ========================\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    TimeSeriesSplit,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder\n",
    ")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif,\n",
    "    RFECV,\n",
    "    SelectFromModel,\n",
    "    SelectKBest\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    brier_score_loss,\n",
    "    accuracy_score,\n",
    "    log_loss,\n",
    "    classification_report,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ========================\n",
    "# Boosting models\n",
    "# ========================\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ========================\n",
    "# Imbalanced learning\n",
    "# ========================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ========================\n",
    "# Model persistence\n",
    "# ========================\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a78d45-fb52-49b6-b4ea-0d46d29c6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT BOTH SEASONS\n",
    "SEASONS = [2025, 2026]\n",
    "\n",
    "FEATURES_STATS = {\n",
    "    'points_pct': 'PTS%',\n",
    "    'srs': 'SRS',\n",
    "    'goals_for_per_game': 'GF/G',\n",
    "    'goals_against_per_game': 'GA/G',\n",
    "    'power_play_pct': 'PP%',\n",
    "    'pen_kill_pct': 'PK%',\n",
    "    'save_pct': 'SV%',\n",
    "    'shots': 'S',\n",
    "    'shots_against': 'SA'\n",
    "}\n",
    "\n",
    "FEATURES_5V5 = {\n",
    "    'corsi_pct_5on5': 'CF%',\n",
    "    'fenwick_pct_5on5': 'FF%',\n",
    "    'exp_on_goals_for': 'xGF',\n",
    "    'exp_on_goals_against': 'xGA',\n",
    "    'hdsc_for_pct': 'HDCF%',\n",
    "    'pdo': 'PDO'\n",
    "}\n",
    "\n",
    "_clean = lambda n: n.replace('*', '').strip() if n else ''\n",
    "_to_num = lambda x: pd.to_numeric(str(x).replace('%', '').replace(',', '').strip(), errors='coerce')\n",
    "\n",
    "def _find_table_in_comments(soup, table_id: str):\n",
    "    # hockey-reference hides some advanced tables inside HTML comments\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        if table_id in c:\n",
    "            return BeautifulSoup(c, 'html.parser').find('table', id=table_id)\n",
    "    return None\n",
    "\n",
    "def _get_stat_cell(row, data_stat: str):\n",
    "    # keep your fallback logic (lower/upper) for safety\n",
    "    return (\n",
    "        row.find(\"td\", {\"data-stat\": data_stat})\n",
    "        or row.find(\"td\", {\"data-stat\": data_stat.lower()})\n",
    "        or row.find(\"td\", {\"data-stat\": data_stat.upper()})\n",
    "    )\n",
    "\n",
    "def scrape_season(season):\n",
    "    url = f\"https://www.hockey-reference.com/leagues/NHL_{season}.html\"\n",
    "\n",
    "    with webdriver.Chrome() as driver:\n",
    "        driver.get(url)\n",
    "        time.sleep(2.5)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # ---------- TEAM STATS ----------\n",
    "    table_stats = soup.find('table', id='stats')\n",
    "    records = []\n",
    "\n",
    "    for i, row in enumerate(table_stats.find_all('tr')[1:]):\n",
    "        cols = row.find_all('td')\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        team = _clean(cols[0].text)\n",
    "        if team == \"League Average\":\n",
    "            continue\n",
    "\n",
    "        rec = {'Team': team, 'Rank': i + 1}\n",
    "        for k, lbl in FEATURES_STATS.items():\n",
    "            el = _get_stat_cell(row, k)\n",
    "            rec[lbl] = _to_num(el.text) if el else None\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "    df_stats = pd.DataFrame(records)\n",
    "    df_stats[\"Season\"] = season\n",
    "\n",
    "    # ---------- 5v5 ADV (IN COMMENTS) ----------\n",
    "    table_adv = _find_table_in_comments(soup, 'stats_adv')\n",
    "    adv_records = []\n",
    "\n",
    "    # Safety: if hockey-reference changes layout, avoid hard crash\n",
    "    if table_adv is not None:\n",
    "        for row in table_adv.find_all('tr')[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            if not cols:\n",
    "                continue\n",
    "\n",
    "            team = _clean(cols[0].text)\n",
    "            if team == \"League Average\":\n",
    "                continue\n",
    "\n",
    "            rec = {'Team': team}\n",
    "            for k, lbl in FEATURES_5V5.items():\n",
    "                el = _get_stat_cell(row, k)\n",
    "                rec[lbl] = _to_num(el.text) if el else None\n",
    "\n",
    "            adv_records.append(rec)\n",
    "\n",
    "    df_5v5 = pd.DataFrame(adv_records)\n",
    "    df_5v5[\"Season\"] = season\n",
    "\n",
    "    # xGF% only if columns exist (prevents KeyError)\n",
    "    if {\"xGF\", \"xGA\"}.issubset(df_5v5.columns) and not df_5v5.empty:\n",
    "        df_5v5[\"xGF%\"] = (df_5v5[\"xGF\"] / (df_5v5[\"xGF\"] + df_5v5[\"xGA\"]) * 100).round(1)\n",
    "    else:\n",
    "        df_5v5[\"xGF%\"] = np.nan\n",
    "\n",
    "    # ---------- MERGE ----------\n",
    "    df = (\n",
    "        pd.merge(df_stats, df_5v5, on=[\"Team\", \"Season\"], how=\"left\")\n",
    "          .sort_values(\"Rank\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Keep your variables (important for downstream code)\n",
    "df_2025 = scrape_season(2025)\n",
    "df_2026 = scrape_season(2026)\n",
    "\n",
    "# Combine both datasets (same output as before)\n",
    "df_combined = (\n",
    "    pd.concat([df_2025, df_2026], ignore_index=True)\n",
    "      .sort_values([\"Team\", \"Season\"])\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00ddb38-a370-48ed-a70a-f0bea4f8315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "TIMEZONE = \"America/Toronto\"\n",
    "\n",
    "def to_csv_url(url: str) -> str:\n",
    "    return re.sub(r\"/results/(nhl-\\d{4})$\", r\"/download/\\1-UTC.csv\", url)\n",
    "\n",
    "def parse_datetime_column(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\", utc=True, dayfirst=True)\n",
    "    miss = dt.isna()\n",
    "    if miss.any():\n",
    "        dt.loc[miss] = pd.to_datetime(s[miss], errors=\"coerce\", utc=True, dayfirst=False)\n",
    "    return dt\n",
    "\n",
    "# ==========================\n",
    "# TEAM NORMALIZATION SECTION\n",
    "# ==========================\n",
    "TEAM_NAME_MAP = {\n",
    "    \"Montreal Canadiens\": \"Montreal Canadiens\",\n",
    "    \"Montréal Canadiens\": \"Montreal Canadiens\",\n",
    "\n",
    "    \"Utah Hockey Club\": \"Utah Mammoth\",\n",
    "    \"Utah HC\": \"Utah Mammoth\",\n",
    "    \"Utah Hockey C\": \"Utah Mammoth\",\n",
    "    \"Utah Mammoth\": \"Utah Mammoth\",\n",
    "}\n",
    "\n",
    "def normalize_team_name(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    s = str(s).strip()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return TEAM_NAME_MAP.get(s, s)\n",
    "\n",
    "def load_game_results(url: str) -> pd.DataFrame:\n",
    "    csv_url = to_csv_url(url)\n",
    "\n",
    "    # One reliable path: download text then read it\n",
    "    r = requests.get(csv_url, headers=UA, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    df = pd.read_csv(StringIO(r.text))\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    lc = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def _col(*names, default=None):\n",
    "        for n in names:\n",
    "            if n in lc:\n",
    "                return df[lc[n]]\n",
    "        return default\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Date_str\": _col(\"date\"),\n",
    "        \"Home Team\": _col(\"home team\", \"home\"),\n",
    "        \"Away Team\": _col(\"away team\", \"away\"),\n",
    "    })\n",
    "\n",
    "    def _extract_scores(series: pd.Series) -> pd.DataFrame:\n",
    "        return series.astype(str).str.extract(r\"(\\d+)\\s*[-–−]\\s*(\\d+)\")\n",
    "\n",
    "    # Scores (prefer explicit columns if present)\n",
    "    if {\"Home Score\", \"Away Score\"}.issubset(df.columns):\n",
    "        out[\"Home Score\"] = pd.to_numeric(df[\"Home Score\"], errors=\"coerce\")\n",
    "        out[\"Away Score\"] = pd.to_numeric(df[\"Away Score\"], errors=\"coerce\")\n",
    "    elif \"Score\" in df.columns:\n",
    "        s = _extract_scores(df[\"Score\"])\n",
    "        out[\"Home Score\"] = pd.to_numeric(s[0], errors=\"coerce\")\n",
    "        out[\"Away Score\"] = pd.to_numeric(s[1], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"Home Score\"] = pd.NA\n",
    "        out[\"Away Score\"] = pd.NA\n",
    "\n",
    "    # Fill missing from \"result\" if available\n",
    "    if \"result\" in lc:\n",
    "        miss = out[\"Home Score\"].isna() | out[\"Away Score\"].isna()\n",
    "        s = _extract_scores(df[lc[\"result\"]])\n",
    "        out.loc[miss, \"Home Score\"] = pd.to_numeric(s[0], errors=\"coerce\")\n",
    "        out.loc[miss, \"Away Score\"] = pd.to_numeric(s[1], errors=\"coerce\")\n",
    "\n",
    "    out[\"Date_UTC\"] = parse_datetime_column(out[\"Date_str\"])\n",
    "    out[\"Date_Local\"] = out[\"Date_UTC\"].dt.tz_convert(TIMEZONE)\n",
    "    out[\"LocalDate\"] = out[\"Date_Local\"].dt.date\n",
    "\n",
    "    # Clean rows\n",
    "    out = (\n",
    "        out.dropna(subset=[\"Date_UTC\"])\n",
    "           .sort_values(\"Date_UTC\")\n",
    "           .drop_duplicates(subset=[\"Date_UTC\", \"Home Team\", \"Away Team\"], keep=\"last\")\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Normalize TEAM NAMES exactly ONCE\n",
    "    out[[\"Home Team\", \"Away Team\"]] = out[[\"Home Team\", \"Away Team\"]].applymap(normalize_team_name)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0140f76-bad9-4cce-84ab-2705257e6cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\AppData\\Local\\Temp\\ipykernel_25912\\2439047771.py:93: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  out[[\"Home Team\", \"Away Team\"]] = out[[\"Home Team\", \"Away Team\"]].applymap(normalize_team_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ df_all built: (2624, 11)\n",
      "✔ df_games_master built: (2624, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\AppData\\Local\\Temp\\ipykernel_25912\\2439047771.py:93: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  out[[\"Home Team\", \"Away Team\"]] = out[[\"Home Team\", \"Away Team\"]].applymap(normalize_team_name)\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# ==== BUILD df_all AFTER loading game results ====\n",
    "# ================================================\n",
    "\n",
    "def _load_season(url: str, season_label: str, weight: float) -> pd.DataFrame:\n",
    "    df = load_game_results(url)\n",
    "    df[\"Season\"] = season_label\n",
    "    df[\"weight\"] = weight\n",
    "    return df\n",
    "\n",
    "df_2024 = _load_season(\"https://fixturedownload.com/results/nhl-2024\", \"2024-2025\", 1.0)\n",
    "df_2025 = _load_season(\"https://fixturedownload.com/results/nhl-2025\", \"2025-2026\", 2.0)\n",
    "\n",
    "df_all = pd.concat([df_2024, df_2025], ignore_index=True)\n",
    "\n",
    "# Played flag\n",
    "df_all[\"Played\"] = df_all[[\"Home Score\", \"Away Score\"]].notna().all(axis=1)\n",
    "\n",
    "# Build master list\n",
    "df_games_master = (\n",
    "    df_all[[\"LocalDate\", \"Home Team\", \"Away Team\", \"Home Score\", \"Away Score\", \"Season\", \"Played\"]]\n",
    "      .rename(columns={\"LocalDate\": \"Date\"})\n",
    "      .sort_values(\"Date\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"✔ df_all built:\", df_all.shape)\n",
    "print(\"✔ df_games_master built:\", df_games_master.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ac4720-ca45-4059-81f6-1abda6d891e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ df_final built: (2220, 12)\n",
      "              Date_str             Home Team            Away Team Home Score  \\\n",
      "2215  06/02/2026 00:30   Tampa Bay Lightning     Florida Panthers        6.0   \n",
      "2216  06/02/2026 00:00   Philadelphia Flyers      Ottawa Senators        1.0   \n",
      "2217  06/02/2026 00:00   Washington Capitals  Nashville Predators        4.0   \n",
      "2218  06/02/2026 00:00        Buffalo Sabres  Pittsburgh Penguins        2.0   \n",
      "2219  06/02/2026 03:00  Vegas Golden Knights    Los Angeles Kings        4.0   \n",
      "\n",
      "     Away Score                  Date_UTC                Date_Local  \\\n",
      "2215        1.0 2026-02-06 00:30:00+00:00 2026-02-05 19:30:00-05:00   \n",
      "2216        2.0 2026-02-06 00:00:00+00:00 2026-02-05 19:00:00-05:00   \n",
      "2217        2.0 2026-02-06 00:00:00+00:00 2026-02-05 19:00:00-05:00   \n",
      "2218        5.0 2026-02-06 00:00:00+00:00 2026-02-05 19:00:00-05:00   \n",
      "2219        1.0 2026-02-06 03:00:00+00:00 2026-02-05 22:00:00-05:00   \n",
      "\n",
      "       LocalDate     Season  weight  Played       Date  \n",
      "2215  2026-02-05  2025-2026     2.0    True 2026-02-05  \n",
      "2216  2026-02-05  2025-2026     2.0    True 2026-02-05  \n",
      "2217  2026-02-05  2025-2026     2.0    True 2026-02-05  \n",
      "2218  2026-02-05  2025-2026     2.0    True 2026-02-05  \n",
      "2219  2026-02-05  2025-2026     2.0    True 2026-02-05  \n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# ==== BUILD df_final (Played only) ===\n",
    "# =====================================\n",
    "\n",
    "df_final = (\n",
    "    df_all.loc[df_all[\"Played\"]]\n",
    "          .copy()\n",
    "          .assign(Date=lambda d: pd.to_datetime(d[\"LocalDate\"], errors=\"coerce\", utc=True).dt.tz_convert(None))\n",
    "          .dropna(subset=[\"Date\"])\n",
    "          .sort_values(\"Date\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"✔ df_final built:\", df_final.shape)\n",
    "print(df_final.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1682fe-26a9-4a15-92ba-893ccb32f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Home Win flag if missing\n",
    "if \"Home Win\" not in df_final.columns:\n",
    "    df_final[\"Home Win\"] = (df_final[\"Home Score\"] > df_final[\"Away Score\"]).astype(int)\n",
    "\n",
    "df_final = df_final.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "OFFSEASON_GAP_DAYS = 45\n",
    "\n",
    "# Pre-allocate outputs (keep same column names)\n",
    "home_last10, away_last10 = [], []\n",
    "home_py, away_py = [], []\n",
    "home_wr, away_wr = [], []\n",
    "home_streak, away_streak = [], []\n",
    "home_rest, away_rest = [], []\n",
    "\n",
    "# State per (team, season)\n",
    "wins_last10 = {}      # key -> list of last wins (0/1)\n",
    "last_date = {}        # key -> last game date (normalized)\n",
    "wins_games = {}       # key -> (wins, games)\n",
    "streak = {}           # key -> current win streak\n",
    "\n",
    "for _, row in df_final.iterrows():\n",
    "    season = row[\"Season\"]\n",
    "    date = pd.Timestamp(row[\"Date\"]).normalize()\n",
    "\n",
    "    home = row[\"Home Team\"]\n",
    "    away = row[\"Away Team\"]\n",
    "    home_key = (home, season)\n",
    "    away_key = (away, season)\n",
    "\n",
    "    home_win = int(row[\"Home Win\"])\n",
    "    away_win = 1 - home_win\n",
    "\n",
    "    # --- 1) LAST 10 WINS (before game) ---\n",
    "    home_last10.append(sum(wins_last10.get(home_key, [])[-10:]))\n",
    "    away_last10.append(sum(wins_last10.get(away_key, [])[-10:]))\n",
    "\n",
    "    # --- 2) PLAYED YESTERDAY ---\n",
    "    yday = date - pd.Timedelta(days=1)\n",
    "    home_py.append(last_date.get(home_key) == yday)\n",
    "    away_py.append(last_date.get(away_key) == yday)\n",
    "\n",
    "    # --- 3) WIN RATE (before game) ---\n",
    "    hw, hg = wins_games.get(home_key, (0, 0))\n",
    "    aw, ag = wins_games.get(away_key, (0, 0))\n",
    "    home_wr.append(hw / hg if hg else 0)\n",
    "    away_wr.append(aw / ag if ag else 0)\n",
    "\n",
    "    # --- 4) OVERALL WIN STREAK (before game) ---\n",
    "    home_streak.append(streak.get(home_key, 0))\n",
    "    away_streak.append(streak.get(away_key, 0))\n",
    "\n",
    "    # --- 5) REST DAYS ---\n",
    "    ph = last_date.get(home_key)\n",
    "    pa = last_date.get(away_key)\n",
    "    dh = (date - ph).days if ph is not None else None\n",
    "    da = (date - pa).days if pa is not None else None\n",
    "    home_rest.append(dh if dh is not None and dh <= OFFSEASON_GAP_DAYS else np.nan)\n",
    "    away_rest.append(da if da is not None and da <= OFFSEASON_GAP_DAYS else np.nan)\n",
    "\n",
    "    # ===== Update state AFTER computing \"before game\" features =====\n",
    "    wins_last10.setdefault(home_key, []).append(home_win)\n",
    "    wins_last10.setdefault(away_key, []).append(away_win)\n",
    "\n",
    "    last_date[home_key] = date\n",
    "    last_date[away_key] = date\n",
    "\n",
    "    wins_games[home_key] = (hw + home_win, hg + 1)\n",
    "    wins_games[away_key] = (aw + away_win, ag + 1)\n",
    "\n",
    "    if home_win:\n",
    "        streak[home_key] = streak.get(home_key, 0) + 1\n",
    "        streak[away_key] = 0\n",
    "    else:\n",
    "        streak[home_key] = 0\n",
    "        streak[away_key] = streak.get(away_key, 0) + 1\n",
    "\n",
    "# Assign columns (same names as your original code)\n",
    "df_final[\"Home Last 10 Wins\"] = home_last10\n",
    "df_final[\"Away Last 10 Wins\"] = away_last10\n",
    "\n",
    "df_final[\"Home Played Yesterday\"] = home_py\n",
    "df_final[\"Away Played Yesterday\"] = away_py\n",
    "\n",
    "df_final[\"Home Win Rate\"] = home_wr\n",
    "df_final[\"Away Win Rate\"] = away_wr\n",
    "\n",
    "df_final[\"Home Team Overall Win Streak Before Game\"] = home_streak\n",
    "df_final[\"Away Team Overall Win Streak Before Game\"] = away_streak\n",
    "\n",
    "df_final[\"Home Rest Days Since Last Game\"] = home_rest\n",
    "df_final[\"Away Rest Days Since Last Game\"] = away_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6507280b-353c-4375-ac48-415d2b8060dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home_PP% NA rate: 0.018\n",
      "Away_PP% NA rate: 0.018\n",
      "Home_xGF% NA rate: 0.018\n",
      "Away_xGF% NA rate: 0.018\n"
     ]
    }
   ],
   "source": [
    "# --- Diffs (OK) ---\n",
    "df_final[\"Home Advantage\"] = df_final[\"Home Win Rate\"] - df_final[\"Away Win Rate\"]\n",
    "df_final[\"Win Streak Impact\"] = (\n",
    "    df_final[\"Home Team Overall Win Streak Before Game\"]\n",
    "    - df_final[\"Away Team Overall Win Streak Before Game\"]\n",
    ")\n",
    "df_final[\"Last 10 Wins\"] = df_final[\"Home Last 10 Wins\"] - df_final[\"Away Last 10 Wins\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MERGE CLEAN (Season_Key + Team_Key)  ✅\n",
    "# ============================================================\n",
    "\n",
    "def _team_key(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "def _season_key(x):\n",
    "    # \"2024-2025\" -> 2025 ; \"2025-2026\" -> 2026 ; 2025 -> 2025\n",
    "    if x is None:\n",
    "        return None\n",
    "    xs = str(x)\n",
    "    if \"-\" in xs:\n",
    "        try:\n",
    "            return int(xs.split(\"-\")[1])\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        return int(float(xs))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- 1) Keys côté df_final ---\n",
    "df_final[\"Season_Key\"] = df_final[\"Season\"].map(_season_key)\n",
    "df_final[\"Home_Team_Key\"] = df_final[\"Home Team\"].map(_team_key)\n",
    "df_final[\"Away_Team_Key\"] = df_final[\"Away Team\"].map(_team_key)\n",
    "df_final = df_final.dropna(subset=[\"Season_Key\", \"Home_Team_Key\", \"Away_Team_Key\"])\n",
    "\n",
    "# --- 2) Keys côté df_combined ---\n",
    "stats = df_combined.copy()\n",
    "stats[\"Season_Key\"] = stats[\"Season\"].map(_season_key)\n",
    "stats[\"Team_Key\"] = stats[\"Team\"].map(_team_key)\n",
    "\n",
    "# garder colonnes utiles + enlever doublons\n",
    "stats = (\n",
    "    stats.drop(columns=[c for c in [\"Season\", \"Team\"] if c in stats.columns])\n",
    "         .drop_duplicates(subset=[\"Season_Key\", \"Team_Key\"])\n",
    ")\n",
    "\n",
    "def _merge_side(df, side: str, key_col: str) -> pd.DataFrame:\n",
    "    side_stats = (\n",
    "        stats.add_prefix(f\"{side}_\")\n",
    "             .rename(columns={f\"{side}_Season_Key\": \"Season_Key\", f\"{side}_Team_Key\": key_col})\n",
    "    )\n",
    "    return df.merge(side_stats, on=[\"Season_Key\", key_col], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "# --- 3-4) Merge Home + Away ---\n",
    "df_final = _merge_side(df_final, \"Home\", \"Home_Team_Key\")\n",
    "df_final = _merge_side(df_final, \"Away\", \"Away_Team_Key\")\n",
    "\n",
    "# --- 5) Quick check ---\n",
    "cols_check = [\"Home_PP%\", \"Away_PP%\", \"Home_xGF%\", \"Away_xGF%\"]\n",
    "for col in cols_check:\n",
    "    if col in df_final.columns:\n",
    "        print(col, \"NA rate:\", df_final[col].isna().mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6091e974-0f5e-42fa-a2a3-eedf1b1c5e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Snapshot saved for 2026-02-25. Total entries: 320\n"
     ]
    }
   ],
   "source": [
    "SNAPSHOT_PATH = \"team_snapshots.csv\"\n",
    "SNAPSHOT_SEASON = 2026\n",
    "\n",
    "today = pd.Timestamp.now().normalize()\n",
    "\n",
    "snapshot = (\n",
    "    df_combined.loc[df_combined[\"Season\"] == SNAPSHOT_SEASON]\n",
    "        .copy()\n",
    "        .assign(\n",
    "            **{\n",
    "                \"Snapshot Date\": today,\n",
    "                \"Team\": lambda d: d[\"Team\"].astype(str).str.strip()\n",
    "            }\n",
    "        )\n",
    ")\n",
    "\n",
    "if os.path.exists(SNAPSHOT_PATH):\n",
    "    df_existing = pd.read_csv(SNAPSHOT_PATH, parse_dates=[\"Snapshot Date\"])\n",
    "    df_existing = df_existing[df_existing[\"Snapshot Date\"] != today]\n",
    "    snapshot = pd.concat([df_existing, snapshot], ignore_index=True)\n",
    "\n",
    "snapshot.to_csv(SNAPSHOT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Snapshot saved for {today.date()}. Total entries: {len(snapshot)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca97918-50e1-4367-9aa7-a0a9b403ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rolling + deltas created.\n",
      "✅ Feature build complete.\n",
      "Snapshot fallback rates: Home= 0.603 Away= 0.605\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FEATURE BUILD (clean + shorter, no leakage)\n",
    "# ============================================================\n",
    "\n",
    "SNAPSHOT_CSV = \"team_snapshots.csv\"\n",
    "USE_DATE_COL = \"Date\"\n",
    "ROLL_WINDOWS = [5, 10, 20]\n",
    "MIN_PERIODS = 3\n",
    "SNAP_METRICS = [\"PP%\", \"PK%\", \"SV%\", \"xGF%\"]\n",
    "\n",
    "# -----------------------\n",
    "# 0) PREP / SAFETY\n",
    "# -----------------------\n",
    "df_final = df_final.loc[:, ~df_final.columns.duplicated()].copy()\n",
    "\n",
    "if USE_DATE_COL not in df_final.columns:\n",
    "    raise ValueError(f\"Missing '{USE_DATE_COL}' in df_final.\")\n",
    "\n",
    "df_final[USE_DATE_COL] = pd.to_datetime(df_final[USE_DATE_COL], errors=\"coerce\")\n",
    "df_final = df_final.dropna(subset=[USE_DATE_COL]).sort_values(USE_DATE_COL).reset_index(drop=True)\n",
    "\n",
    "for c in [\"Home Team\", \"Away Team\", \"Season\"]:\n",
    "    if c not in df_final.columns:\n",
    "        raise ValueError(f\"Missing '{c}' in df_final.\")\n",
    "\n",
    "def _to_float_series(s):\n",
    "    if s.dtype == \"O\":\n",
    "        s = s.astype(str).str.replace(\"%\", \"\", regex=False).str.replace(\",\", \"\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _team_key(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "def _season_key(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    xs = str(x)\n",
    "    if \"-\" in xs:\n",
    "        try:\n",
    "            return int(xs.split(\"-\")[1])\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        return int(float(xs))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def prev_season_label(season_str: str) -> str:\n",
    "    a, b = season_str.split(\"-\")\n",
    "    a, b = int(a), int(b)\n",
    "    return f\"{a-1}-{b-1}\"\n",
    "\n",
    "# -----------------------\n",
    "# 1) LOAD SNAPSHOTS\n",
    "# -----------------------\n",
    "snapshots = pd.read_csv(SNAPSHOT_CSV, parse_dates=[\"Snapshot Date\"])\n",
    "snapshots[\"Team\"] = snapshots[\"Team\"].astype(str).str.strip()\n",
    "snapshots[\"Season\"] = snapshots[\"Season\"].astype(str).str.strip()\n",
    "\n",
    "for m in SNAP_METRICS:\n",
    "    if m in snapshots.columns:\n",
    "        snapshots[m] = _to_float_series(snapshots[m])\n",
    "\n",
    "# -----------------------\n",
    "# 2) BUILD PRIOR FINAL STATS (prev season final snapshot)\n",
    "# -----------------------\n",
    "if {\"Season\", \"Team\"}.issubset(df_combined.columns) is False:\n",
    "    raise ValueError(\"df_combined must contain ['Season','Team', ...]\")\n",
    "\n",
    "final_2025_stats = df_combined.loc[df_combined[\"Season\"] == 2025].copy()\n",
    "final_2025_stats[\"Team\"] = final_2025_stats[\"Team\"].astype(str).str.strip()\n",
    "final_2025_stats[\"Snapshot Date\"] = pd.Timestamp(\"2025-07-01\")\n",
    "final_2025_stats[\"Season\"] = \"2024-2025\"\n",
    "\n",
    "keep_cols = [\"Team\", \"Season\", \"Snapshot Date\"] + [\n",
    "    c for c in final_2025_stats.columns\n",
    "    if c in SNAP_METRICS or c in [\"Rank\",\"PTS%\",\"SRS\",\"GF/G\",\"GA/G\",\"S\",\"SA\",\"CF%\",\"FF%\",\"xGF\",\"xGA\",\"HDCF%\",\"PDO\",\"xGF%\"]\n",
    "]\n",
    "keep_cols = list(dict.fromkeys(keep_cols))\n",
    "final_2025_stats = final_2025_stats.loc[:, keep_cols]\n",
    "\n",
    "for m in SNAP_METRICS:\n",
    "    if m in final_2025_stats.columns:\n",
    "        final_2025_stats[m] = _to_float_series(final_2025_stats[m])\n",
    "\n",
    "# -----------------------\n",
    "# 3) MERGE SNAPSHOTS (NO-LEAK) via merge_asof\n",
    "# -----------------------\n",
    "def merge_snapshot_stats(df_games, df_snaps, prior_final_stats, team_col, prefix):\n",
    "    df_games = df_games.copy()\n",
    "    df_games[\"Season\"] = df_games[\"Season\"].astype(str).str.strip()\n",
    "    df_games[team_col] = df_games[team_col].astype(str).str.strip()\n",
    "\n",
    "    # Build keys for stable joins\n",
    "    df_games[f\"{prefix}_Team_Key\"] = df_games[team_col].map(_team_key)\n",
    "    df_snaps = df_snaps.copy()\n",
    "    df_snaps[\"Team_Key\"] = df_snaps[\"Team\"].map(_team_key)\n",
    "\n",
    "    # Same-season snapshots (<= game date)\n",
    "    left = df_games[[USE_DATE_COL, \"Season\", f\"{prefix}_Team_Key\"]].rename(columns={f\"{prefix}_Team_Key\": \"Team_Key\"})\n",
    "    left = left.sort_values(USE_DATE_COL)\n",
    "\n",
    "    right = df_snaps.rename(columns={\"Snapshot Date\": \"SnapDate\"}).sort_values(\"SnapDate\")\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        left,\n",
    "        right,\n",
    "        left_on=USE_DATE_COL,\n",
    "        right_on=\"SnapDate\",\n",
    "        by=[\"Season\", \"Team_Key\"],\n",
    "        direction=\"backward\",\n",
    "        allow_exact_matches=True\n",
    "    )\n",
    "\n",
    "    # Fallback: previous season final only for the specific known case (your original rule)\n",
    "    fallback = prior_final_stats.copy()\n",
    "    fallback[\"Team_Key\"] = fallback[\"Team\"].map(_team_key)\n",
    "\n",
    "    # Determine which rows need fallback\n",
    "    need = merged[\"SnapDate\"].isna() & merged[\"Season\"].str.contains(\"-\", na=False)\n",
    "\n",
    "    if need.any():\n",
    "        prev = merged.loc[need, \"Season\"].map(prev_season_label)\n",
    "        # only apply fallback when prev season == \"2024-2025\" (your exact behavior)\n",
    "        use_fb = prev.eq(\"2024-2025\")\n",
    "        idx = merged.loc[need].index[use_fb]\n",
    "\n",
    "        if len(idx):\n",
    "            fb_pick = merged.loc[idx, [\"Team_Key\"]].merge(\n",
    "                fallback, on=\"Team_Key\", how=\"left\"\n",
    "            )\n",
    "            # overwrite missing snapshot fields with fallback fields\n",
    "            for c in fallback.columns:\n",
    "                if c in [\"Team_Key\"]:\n",
    "                    continue\n",
    "                merged.loc[idx, c] = fb_pick[c].values\n",
    "            merged.loc[idx, \"SnapDate\"] = fb_pick[\"Snapshot Date\"].values\n",
    "\n",
    "    # Add fallback flag: 1 if still missing OR if filled by fallback\n",
    "    used_fallback = merged[\"SnapDate\"].isna().astype(int)\n",
    "    # (If you want to count “filled by fallback” as 1 too:)\n",
    "    # used_fallback |= need.astype(int)  # optional, but your original flagged only when used\n",
    "\n",
    "    # Prefix columns (except keys)\n",
    "    keep = [c for c in merged.columns if c not in [\"Season\", \"Team_Key\", USE_DATE_COL]]\n",
    "    pref = merged[keep].rename(columns=lambda c: f\"{prefix}_{c}\")\n",
    "    out = pd.concat([df_games, pref], axis=1)\n",
    "    out[f\"{prefix}_Snapshot_Fallback\"] = used_fallback.values\n",
    "    return out\n",
    "\n",
    "df_final = merge_snapshot_stats(df_final, snapshots, final_2025_stats, \"Home Team\", \"Home\")\n",
    "df_final = merge_snapshot_stats(df_final, snapshots, final_2025_stats, \"Away Team\", \"Away\")\n",
    "df_final = df_final.loc[:, ~df_final.columns.duplicated()].copy()\n",
    "\n",
    "# Ensure required snapshot metric columns exist\n",
    "missing_cols = [f\"{side}_{m}\" for side in [\"Home\", \"Away\"] for m in SNAP_METRICS if f\"{side}_{m}\" not in df_final.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required snapshot columns after merge: {missing_cols}\")\n",
    "\n",
    "# -----------------------\n",
    "# 4) ROLLING RECENT FORM (SHIFTED) + DELTAS\n",
    "# -----------------------\n",
    "for m in SNAP_METRICS:\n",
    "    df_final[f\"Home_{m}\"] = _to_float_series(df_final[f\"Home_{m}\"])\n",
    "    df_final[f\"Away_{m}\"] = _to_float_series(df_final[f\"Away_{m}\"])\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "df_final[\"Game_ID\"] = df_final.index\n",
    "\n",
    "def _side_long(side: str, team_col: str):\n",
    "    cols = [\"Game_ID\", USE_DATE_COL, \"Season\", team_col] + [f\"{side}_{m}\" for m in SNAP_METRICS]\n",
    "    out = df_final[cols].copy().rename(columns={team_col: \"Team\"})\n",
    "    out[\"Side\"] = side\n",
    "    for m in SNAP_METRICS:\n",
    "        out = out.rename(columns={f\"{side}_{m}\": m})\n",
    "    return out\n",
    "\n",
    "team_long = pd.concat(\n",
    "    [_side_long(\"Home\", \"Home Team\"), _side_long(\"Away\", \"Away Team\")],\n",
    "    ignore_index=True\n",
    ").sort_values([\"Team\", USE_DATE_COL, \"Game_ID\"]).reset_index(drop=True)\n",
    "\n",
    "for m in SNAP_METRICS:\n",
    "    shifted = team_long.groupby(\"Team\")[m].shift(1)\n",
    "    for w in ROLL_WINDOWS:\n",
    "        team_long[f\"{m}_L{w}\"] = shifted.groupby(team_long[\"Team\"]).transform(\n",
    "            lambda s: s.rolling(w, min_periods=MIN_PERIODS).mean()\n",
    "        )\n",
    "\n",
    "roll_cols = [f\"{m}_L{w}\" for m in SNAP_METRICS for w in ROLL_WINDOWS]\n",
    "\n",
    "cols_to_drop = (\n",
    "    [f\"Home_{c}\" for c in roll_cols] + [f\"Away_{c}\" for c in roll_cols] +\n",
    "    [f\"Delta_{m}_L{w}\" for m in SNAP_METRICS for w in ROLL_WINDOWS]\n",
    ")\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns], errors=\"ignore\")\n",
    "\n",
    "home_roll = team_long.loc[team_long[\"Side\"] == \"Home\", [\"Game_ID\"] + roll_cols].rename(columns={c: f\"Home_{c}\" for c in roll_cols})\n",
    "away_roll = team_long.loc[team_long[\"Side\"] == \"Away\", [\"Game_ID\"] + roll_cols].rename(columns={c: f\"Away_{c}\" for c in roll_cols})\n",
    "\n",
    "df_final = df_final.merge(home_roll, on=\"Game_ID\", how=\"left\").merge(away_roll, on=\"Game_ID\", how=\"left\")\n",
    "\n",
    "for m in SNAP_METRICS:\n",
    "    for w in ROLL_WINDOWS:\n",
    "        df_final[f\"Delta_{m}_L{w}\"] = df_final[f\"Home_{m}_L{w}\"] - df_final[f\"Away_{m}_L{w}\"]\n",
    "\n",
    "# Drop raw snapshot metrics (optional)\n",
    "drop_snapshot_cols = [f\"Home_{m}\" for m in SNAP_METRICS] + [f\"Away_{m}\" for m in SNAP_METRICS]\n",
    "df_final = df_final.drop(columns=[c for c in drop_snapshot_cols if c in df_final.columns], errors=\"ignore\")\n",
    "\n",
    "print(\"✅ Rolling + deltas created.\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) EXTRA FEATURES (Rank/SRS diffs + Opponent Strength)\n",
    "# -----------------------\n",
    "if {\"Home_Rank\", \"Away_Rank\"}.issubset(df_final.columns):\n",
    "    df_final[\"Rank Difference\"] = _to_float_series(df_final[\"Home_Rank\"]) - _to_float_series(df_final[\"Away_Rank\"])\n",
    "\n",
    "if {\"Home_SRS\", \"Away_SRS\"}.issubset(df_final.columns):\n",
    "    df_final[\"SRS_Diff\"] = _to_float_series(df_final[\"Home_SRS\"]) - _to_float_series(df_final[\"Away_SRS\"])\n",
    "\n",
    "def calculate_avg_opponent_rank(df):\n",
    "    hist = {}\n",
    "    home_strengths, away_strengths = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        h, a = str(row[\"Home Team\"]), str(row[\"Away Team\"])\n",
    "        hist.setdefault(h, [])\n",
    "        hist.setdefault(a, [])\n",
    "\n",
    "        home_strengths.append(np.mean(hist[h][-10:]) if hist[h] else np.nan)\n",
    "        away_strengths.append(np.mean(hist[a][-10:]) if hist[a] else np.nan)\n",
    "\n",
    "        ar = row.get(\"Away_Rank\", np.nan)\n",
    "        hr = row.get(\"Home_Rank\", np.nan)\n",
    "        if not pd.isna(ar):\n",
    "            hist[h].append(float(ar))\n",
    "        if not pd.isna(hr):\n",
    "            hist[a].append(float(hr))\n",
    "    return home_strengths, away_strengths\n",
    "\n",
    "df_final = df_final.sort_values(USE_DATE_COL).reset_index(drop=True)\n",
    "df_final[\"Home Opponent Strength\"], df_final[\"Away Opponent Strength\"] = calculate_avg_opponent_rank(df_final)\n",
    "\n",
    "print(\"✅ Feature build complete.\")\n",
    "print(\"Snapshot fallback rates:\",\n",
    "      \"Home=\", round(df_final[\"Home_Snapshot_Fallback\"].mean(), 3),\n",
    "      \"Away=\", round(df_final[\"Away_Snapshot_Fallback\"].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94cc9efd-60bf-4ac9-9966-0f9f4c1048c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Walk-forward Feature Set ===\n",
      "WINDOW_MODE: L5\n",
      "CAL_MODE:    sigmoid\n",
      "Rolling deltas: 4\n",
      "Diffs:          5 -> ['SRS_Diff', 'Rank Difference', 'Win Streak Impact', 'Last 10 Wins', 'Home Advantage']\n",
      "Opp strength:   2 -> ['Home Opponent Strength', 'Away Opponent Strength']\n",
      "Rest/B2B:       2 -> ['Rest_Diff', 'B2B_Diff']\n",
      "TOTAL:          13\n",
      "\n",
      "=== WALK-FORWARD (overall) ===\n",
      "Brier:   0.23609\n",
      "ROC-AUC: 0.62641\n",
      "LogLoss: 0.66486\n",
      "\n",
      "=== Brier by Month ===\n",
      "  Month    Brier\n",
      "2024-12 0.204426\n",
      "2025-01 0.246612\n",
      "2025-02 0.240998\n",
      "2025-03 0.219697\n",
      "2025-04 0.231825\n",
      "2025-10 0.247093\n",
      "2025-11 0.240715\n",
      "2025-12 0.248881\n",
      "2026-01 0.233843\n",
      "2026-02 0.188748\n",
      "\n",
      "=== Calibration bins ===\n",
      "       bin    n   p_mean  win_rate\n",
      "(0.1, 0.2]   15 0.195402  0.000000\n",
      "(0.2, 0.3]  227 0.264727  0.387665\n",
      "(0.3, 0.4] 2032 0.360657  0.397146\n",
      "(0.4, 0.5] 4738 0.454367  0.440692\n",
      "(0.5, 0.6] 6681 0.550519  0.550666\n",
      "(0.6, 0.7] 5505 0.643392  0.642507\n",
      "(0.7, 0.8] 1888 0.736333  0.724047\n",
      "(0.8, 0.9]  182 0.828320  0.763736\n",
      "\n",
      "✅ Saved walk-forward outputs with tag: L5_sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\AppData\\Local\\Temp\\ipykernel_25912\\3485064123.py:141: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: brier_score_loss(g[\"y_true\"], g[\"p_home\"]))\n"
     ]
    }
   ],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "DATE_COL = \"Date\"\n",
    "TARGET_COL = \"Home Win\"\n",
    "TEST_HORIZON_DAYS = 14\n",
    "MIN_TRAIN_GAMES = 500\n",
    "CALIBRATION_BINS = 10\n",
    "\n",
    "WINDOW_MODE = \"L5\"        # \"L5\" | \"L10\" | \"L20\" | \"L5_L20\" | \"ALL\"\n",
    "CAL_MODE = \"sigmoid\"      # \"none\" | \"sigmoid\" | \"isotonic\"\n",
    "CAL_CV = 3\n",
    "\n",
    "# ---------------- BUILD BACKTEST DF ----------------\n",
    "df_bt = (\n",
    "    df_final.loc[:, ~df_final.columns.duplicated()]\n",
    "            .copy()\n",
    ")\n",
    "\n",
    "df_bt[DATE_COL] = pd.to_datetime(df_bt[DATE_COL], errors=\"coerce\")\n",
    "df_bt = df_bt.dropna(subset=[DATE_COL, TARGET_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "# keep played only (scores present)\n",
    "if {\"Home Score\", \"Away Score\"}.issubset(df_bt.columns):\n",
    "    df_bt = df_bt.dropna(subset=[\"Home Score\", \"Away Score\"]).reset_index(drop=True)\n",
    "\n",
    "# Build diffs\n",
    "if {\"Home Rest Days Since Last Game\", \"Away Rest Days Since Last Game\"}.issubset(df_bt.columns):\n",
    "    df_bt[\"Rest_Diff\"] = df_bt[\"Home Rest Days Since Last Game\"] - df_bt[\"Away Rest Days Since Last Game\"]\n",
    "\n",
    "if {\"Home Played Yesterday\", \"Away Played Yesterday\"}.issubset(df_bt.columns):\n",
    "    df_bt[\"B2B_Diff\"] = df_bt[\"Home Played Yesterday\"].astype(int) - df_bt[\"Away Played Yesterday\"].astype(int)\n",
    "\n",
    "# ---- Rolling delta selection ----\n",
    "all_delta = sorted([c for c in df_bt.columns if c.startswith(\"Delta_\") and \"_L\" in c])\n",
    "\n",
    "_mode_map = {\n",
    "    \"L5\":      [\"_L5\"],\n",
    "    \"L10\":     [\"_L10\"],\n",
    "    \"L20\":     [\"_L20\"],\n",
    "    \"L5_L20\":  [\"_L5\", \"_L20\"],\n",
    "    \"ALL\":     None\n",
    "}\n",
    "if WINDOW_MODE not in _mode_map:\n",
    "    raise ValueError(\"Bad WINDOW_MODE\")\n",
    "\n",
    "need = _mode_map[WINDOW_MODE]\n",
    "FEAT_ROLLING_DELTAS = all_delta if need is None else [c for c in all_delta if any(t in c for t in need)]\n",
    "\n",
    "# Other features\n",
    "FEAT_DIFFS = [c for c in [\"SRS_Diff\", \"Rank Difference\", \"Win Streak Impact\", \"Last 10 Wins\", \"Home Advantage\"] if c in df_bt.columns]\n",
    "FEAT_OPP   = [c for c in [\"Home Opponent Strength\", \"Away Opponent Strength\"] if c in df_bt.columns]\n",
    "FEAT_REST  = [c for c in [\"Rest_Diff\", \"B2B_Diff\"] if c in df_bt.columns]\n",
    "\n",
    "feature_cols = list(dict.fromkeys(FEAT_ROLLING_DELTAS + FEAT_DIFFS + FEAT_OPP + FEAT_REST))\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"No features selected. Ensure Delta_*_L* exists (and WINDOW_MODE matches).\")\n",
    "\n",
    "print(\"\\n=== Walk-forward Feature Set ===\")\n",
    "print(\"WINDOW_MODE:\", WINDOW_MODE)\n",
    "print(\"CAL_MODE:   \", CAL_MODE)\n",
    "print(f\"Rolling deltas: {len(FEAT_ROLLING_DELTAS)}\")\n",
    "print(f\"Diffs:          {len(FEAT_DIFFS)} -> {FEAT_DIFFS}\")\n",
    "print(f\"Opp strength:   {len(FEAT_OPP)} -> {FEAT_OPP}\")\n",
    "print(f\"Rest/B2B:       {len(FEAT_REST)} -> {FEAT_REST}\")\n",
    "print(f\"TOTAL:          {len(feature_cols)}\")\n",
    "\n",
    "# X/y\n",
    "y_all = df_bt[TARGET_COL].astype(int).to_numpy()\n",
    "X_all = df_bt[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ============================================================\n",
    "# DROP GLOBAL ALL-NaN FEATURES (CRITICAL INTEGRITY FIX)\n",
    "# ============================================================\n",
    "global_all_nan = X_all.columns[X_all.isna().all()].tolist()\n",
    "if global_all_nan:\n",
    "    print(\"Dropping globally all-NaN features:\", global_all_nan)\n",
    "    X_all = X_all.drop(columns=global_all_nan)\n",
    "    feature_cols = [c for c in feature_cols if c not in global_all_nan]\n",
    "\n",
    "# ---------------- MODEL PIPELINE ----------------\n",
    "base_clf = LogisticRegression(max_iter=4000, solver=\"lbfgs\", C=1.0, random_state=42)\n",
    "\n",
    "if CAL_MODE == \"none\":\n",
    "    clf = base_clf\n",
    "elif CAL_MODE in (\"sigmoid\", \"isotonic\"):\n",
    "    clf = CalibratedClassifierCV(base_clf, method=CAL_MODE, cv=CAL_CV)\n",
    "else:\n",
    "    raise ValueError(\"Bad CAL_MODE\")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", clf),\n",
    "])\n",
    "\n",
    "# ---------------- WALK-FORWARD ----------------\n",
    "pred_rows = []\n",
    "dates = df_bt[DATE_COL]\n",
    "\n",
    "# unique start days (sorted)\n",
    "unique_days = sorted(dates.dt.normalize().unique())\n",
    "\n",
    "for ws in unique_days:\n",
    "    we = ws + pd.Timedelta(days=TEST_HORIZON_DAYS)\n",
    "\n",
    "    train_mask = dates < ws\n",
    "    test_mask  = (dates >= ws) & (dates < we)\n",
    "\n",
    "    if test_mask.sum() == 0 or train_mask.sum() < MIN_TRAIN_GAMES:\n",
    "        continue\n",
    "\n",
    "    X_tr, y_tr = X_all.loc[train_mask], y_all[train_mask]\n",
    "    X_te, y_te = X_all.loc[test_mask],  y_all[test_mask]\n",
    "\n",
    "    if X_tr.shape[1] == 0:\n",
    "        continue\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    p = np.clip(pipe.predict_proba(X_te)[:, 1], 1e-6, 1 - 1e-6)\n",
    "\n",
    "    block = df_bt.loc[test_mask, [DATE_COL, \"Home Team\", \"Away Team\", \"Season\"]].copy()\n",
    "    block[\"y_true\"] = y_te\n",
    "    block[\"p_home\"] = p\n",
    "    pred_rows.append(block)\n",
    "\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True) if pred_rows else pd.DataFrame()\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "overall_brier = brier_score_loss(pred_df[\"y_true\"], pred_df[\"p_home\"])\n",
    "overall_roc   = roc_auc_score(pred_df[\"y_true\"], pred_df[\"p_home\"])\n",
    "overall_ll    = log_loss(pred_df[\"y_true\"], pred_df[\"p_home\"])\n",
    "\n",
    "print(\"\\n=== WALK-FORWARD (overall) ===\")\n",
    "print(f\"Brier:   {overall_brier:.5f}\")\n",
    "print(f\"ROC-AUC: {overall_roc:.5f}\")\n",
    "print(f\"LogLoss: {overall_ll:.5f}\")\n",
    "\n",
    "# Brier by month\n",
    "pred_df[\"Month\"] = pred_df[DATE_COL].dt.to_period(\"M\").astype(str)\n",
    "brier_by_month = (\n",
    "    pred_df.groupby(\"Month\")\n",
    "           .apply(lambda g: brier_score_loss(g[\"y_true\"], g[\"p_home\"]))\n",
    "           .reset_index(name=\"Brier\")\n",
    ")\n",
    "\n",
    "# Calibration bins\n",
    "pred_df[\"bin\"] = pd.cut(pred_df[\"p_home\"], bins=np.linspace(0, 1, CALIBRATION_BINS + 1), include_lowest=True)\n",
    "cal_table = (\n",
    "    pred_df.groupby(\"bin\", observed=True)\n",
    "           .agg(n=(\"y_true\", \"size\"), p_mean=(\"p_home\", \"mean\"), win_rate=(\"y_true\", \"mean\"))\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Brier by Month ===\")\n",
    "print(brier_by_month.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Calibration bins ===\")\n",
    "print(cal_table.to_string(index=False))\n",
    "\n",
    "# Save outputs with tags so you can compare runs quickly\n",
    "tag = f\"{WINDOW_MODE}_{CAL_MODE}\"\n",
    "pred_df.to_csv(f\"walkforward_predictions_{tag}.csv\", index=False)\n",
    "brier_by_month.to_csv(f\"walkforward_brier_by_month_{tag}.csv\", index=False)\n",
    "cal_table.to_csv(f\"walkforward_calibration_bins_{tag}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✅ Saved walk-forward outputs with tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bac0a76-fe65-4aea-9ebf-5e556e0e2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot fallback rates: Home=0.603 Away=0.605\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Snapshot fallback rates:\",\n",
    "    *(f\"{side}={df_final[f'{side}_Snapshot_Fallback'].mean():.3f}\"\n",
    "      for side in [\"Home\", \"Away\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5245cfd9-5b3f-4445-86f0-4df61557dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 50 candidates, totalling 300 fits\n",
      "Fitting 6 folds for each of 60 candidates, totalling 360 fits\n",
      "\n",
      "Pre-calibration best: ElasticNet {'F1': 0.5804263371794003, 'ROC_AUC': np.float64(0.6412637139374238), 'Brier': np.float64(0.23691156774842256)}\n",
      "\n",
      "Calibration comparison:\n",
      "base      Brier=0.23691  ROC=0.64126  F1=0.58043\n",
      "sigmoid   Brier=0.23666  ROC=0.64325  F1=0.59169\n",
      "isotonic  Brier=0.24040  ROC=0.64162  F1=0.57334\n",
      "\n",
      "✅ FINAL: ElasticNet + sigmoid\n",
      "Final metrics: {'F1': 0.5916916916916917, 'ROC_AUC': np.float64(0.6432547744819178), 'Brier': np.float64(0.23665720734446066)}\n",
      "\n",
      "=== Results ===\n",
      "                     Model      F1  ROC_AUC   Brier\n",
      "FINAL ElasticNet + sigmoid 0.59169  0.64325 0.23666\n",
      "                ElasticNet 0.58043  0.64126 0.23691\n",
      "               Logistic L2 0.59034  0.63149 0.23888\n",
      "✅ Saved model metadata: model_LOGISTIC_PHASE1_FINAL_meta.joblib\n",
      "\n",
      "✅ Saved final model + summary.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL TRAINING — Logistic L2 vs ElasticNet (Brier-first)\n",
    "# Requires: feature_cols from Walk-forward cell\n",
    "# ============================================================\n",
    "\n",
    "SEED = 42\n",
    "CV_SPLITS = 6\n",
    "GAP = 3\n",
    "TEST_RATIO = 0.2\n",
    "TARGET_COL = \"Home Win\"\n",
    "USE_TEAM_DUMMIES = True\n",
    "\n",
    "if \"feature_cols\" not in globals() or not feature_cols:\n",
    "    raise RuntimeError(\"feature_cols missing. Run the Walk-forward cell first.\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "df = df_final.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Rebuild Rest/B2B diffs here (they exist in df_bt, not always in df_final)\n",
    "if {\"Home Rest Days Since Last Game\", \"Away Rest Days Since Last Game\"}.issubset(df.columns):\n",
    "    df[\"Rest_Diff\"] = df[\"Home Rest Days Since Last Game\"] - df[\"Away Rest Days Since Last Game\"]\n",
    "if {\"Home Played Yesterday\", \"Away Played Yesterday\"}.issubset(df.columns):\n",
    "    df[\"B2B_Diff\"] = df[\"Home Played Yesterday\"].astype(int) - df[\"Away Played Yesterday\"].astype(int)\n",
    "\n",
    "need_cols = [TARGET_COL, \"Home Score\", \"Away Score\", \"Home Team\", \"Away Team\", \"Date\"]\n",
    "df_base = df.dropna(subset=[c for c in need_cols if c in df.columns]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Keep a clean history object for prediction later (played only)\n",
    "df_hist_for_pred = df_base.copy()\n",
    "\n",
    "# Optional categorical team features\n",
    "cat_features = []\n",
    "if USE_TEAM_DUMMIES:\n",
    "    df_base[\"Home_Team\"] = df_base[\"Home Team\"].astype(\"category\")\n",
    "    df_base[\"Away_Team\"] = df_base[\"Away Team\"].astype(\"category\")\n",
    "    cat_features = [\"Home_Team\", \"Away_Team\"]\n",
    "\n",
    "# Rename numeric feature cols (spaces -> underscore)\n",
    "rename_map = {c: c.replace(\" \", \"_\") for c in feature_cols}\n",
    "inv_rename_map = {v: k for k, v in rename_map.items()}\n",
    "df_base = df_base.rename(columns=rename_map)\n",
    "feat = [rename_map[c] for c in feature_cols]\n",
    "\n",
    "X = df_base[feat + cat_features]\n",
    "y = df_base[TARGET_COL].astype(int)\n",
    "\n",
    "# Drop global all-NaN numeric features (keep meta aligned)\n",
    "all_nan_global = X[feat].columns[X[feat].isna().all()].tolist()\n",
    "if all_nan_global:\n",
    "    print(\"Dropping all-NaN features:\", all_nan_global)\n",
    "    feat = [c for c in feat if c not in all_nan_global]\n",
    "    X = df_base[feat + cat_features]\n",
    "    dropped_original = [inv_rename_map[c] for c in all_nan_global if c in inv_rename_map]\n",
    "    feature_cols = [c for c in feature_cols if c not in dropped_original]\n",
    "\n",
    "cut = int((1 - TEST_RATIO) * len(df_base))\n",
    "X_tr, X_te = X.iloc[:cut], X.iloc[cut:]\n",
    "y_tr, y_te = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=CV_SPLITS, gap=GAP)\n",
    "\n",
    "# Preprocess: impute->scale for numeric; one-hot for teams\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]), feat),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "def eval_model(model, Xt, yt):\n",
    "    p = model.predict_proba(Xt)[:, 1]\n",
    "    yhat = (p >= 0.5).astype(int)\n",
    "    return {\n",
    "        \"F1\": f1_score(yt, yhat, average=\"weighted\"),\n",
    "        \"ROC_AUC\": roc_auc_score(yt, p),\n",
    "        \"Brier\": brier_score_loss(yt, p),\n",
    "    }\n",
    "\n",
    "def calibrate_and_pick(model, Xtr, ytr, Xte, yte, cv=5):\n",
    "    base_m = eval_model(model, Xte, yte)\n",
    "    cands = [(\"base\", model, base_m)]\n",
    "    for method in [\"sigmoid\", \"isotonic\"]:\n",
    "        cal = CalibratedClassifierCV(model, method=method, cv=cv).fit(Xtr, ytr)\n",
    "        cands.append((method, cal, eval_model(cal, Xte, yte)))\n",
    "    return min(cands, key=lambda x: x[2][\"Brier\"]), cands\n",
    "\n",
    "def run_search(pipe, params, n_iter):\n",
    "    return RandomizedSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"neg_brier_score\",\n",
    "        refit=True,\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=SEED\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "pipe_l2 = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=4000, solver=\"lbfgs\", random_state=SEED)),\n",
    "])\n",
    "\n",
    "pipe_en = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=4000, solver=\"saga\", penalty=\"elasticnet\", random_state=SEED)),\n",
    "])\n",
    "\n",
    "PARAMS_L2 = {\"clf__C\": loguniform(1e-3, 1e1), \"clf__class_weight\": [None, \"balanced\"]}\n",
    "PARAMS_EN = {\"clf__C\": loguniform(1e-3, 1e1), \"clf__l1_ratio\": uniform(0, 1), \"clf__class_weight\": [None, \"balanced\"]}\n",
    "\n",
    "rs_l2 = run_search(pipe_l2, PARAMS_L2, n_iter=50)\n",
    "rs_en = run_search(pipe_en, PARAMS_EN, n_iter=60)\n",
    "\n",
    "best_l2, best_en = rs_l2.best_estimator_, rs_en.best_estimator_\n",
    "\n",
    "m_l2 = eval_model(best_l2, X_te, y_te)\n",
    "m_en = eval_model(best_en, X_te, y_te)\n",
    "\n",
    "chosen = (\"Logistic L2\", best_l2, m_l2) if m_l2[\"Brier\"] <= m_en[\"Brier\"] else (\"ElasticNet\", best_en, m_en)\n",
    "print(\"\\nPre-calibration best:\", chosen[0], chosen[2])\n",
    "\n",
    "(best_cal_name, best_cal_model, best_cal_metrics), cal_cands = calibrate_and_pick(chosen[1], X_tr, y_tr, X_te, y_te, cv=5)\n",
    "\n",
    "print(\"\\nCalibration comparison:\")\n",
    "for name, mdl, met in cal_cands:\n",
    "    print(f\"{name:8s}  Brier={met['Brier']:.5f}  ROC={met['ROC_AUC']:.5f}  F1={met['F1']:.5f}\")\n",
    "\n",
    "print(f\"\\n✅ FINAL: {chosen[0]} + {best_cal_name}\")\n",
    "print(\"Final metrics:\", best_cal_metrics)\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"Model\": \"Logistic L2\", **m_l2},\n",
    "    {\"Model\": \"ElasticNet\", **m_en},\n",
    "    {\"Model\": f\"FINAL {chosen[0]} + {best_cal_name}\", **best_cal_metrics},\n",
    "]).sort_values(\"Brier\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Results ===\")\n",
    "print(results.round(5).to_string(index=False))\n",
    "\n",
    "joblib.dump(best_cal_model, \"model_LOGISTIC_PHASE1_FINAL.joblib\")\n",
    "\n",
    "MODEL_META_PATH = \"model_LOGISTIC_PHASE1_FINAL_meta.joblib\"\n",
    "meta = {\n",
    "    \"feature_cols\": feature_cols,          # original names (before underscore rename)\n",
    "    \"use_team_dummies\": USE_TEAM_DUMMIES,\n",
    "    \"target_col\": TARGET_COL,\n",
    "}\n",
    "joblib.dump(meta, MODEL_META_PATH)\n",
    "print(\"✅ Saved model metadata:\", MODEL_META_PATH)\n",
    "\n",
    "results.to_csv(\"model_results_summary_logistic_only.csv\", index=False)\n",
    "print(\"\\n✅ Saved final model + summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d293965c-875a-40c1-9e52-cfe330c48b6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Target slate date set to: 2026-02-25\n",
      "\n",
      "Target date: 2026-02-25 | games found: 8\n",
      "Nearby slates (games per day):\n",
      "  2026-02-24: 0\n",
      "  2026-02-25: 8  (today)\n",
      "  2026-02-26: 12\n",
      "  2026-02-27: 4\n",
      "  2026-02-28: 13\n",
      "\n",
      "Enter AMERICAN odds (e.g., -120, +135).\n",
      "Press Enter to leave a side blank, 's' to skip a game, 'q' to quit.\n",
      "\n",
      "========================================\n",
      "[1/8]  Edmonton Oilers  @  Anaheim Ducks\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Edmonton Oilers  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "==========================================\n",
      "[2/8]  Winnipeg Jets  @  Vancouver Canucks\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Winnipeg Jets  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "=================================================\n",
      "[3/8]  Vegas Golden Knights  @  Los Angeles Kings\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Vegas Golden Knights  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "==========================================\n",
      "[4/8]  Colorado Avalanche  @  Utah Mammoth\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Colorado Avalanche  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "==================================================\n",
      "[5/8]  Philadelphia Flyers  @  Washington Capitals\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Philadelphia Flyers  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "==================================================\n",
      "[6/8]  Toronto Maple Leafs  @  Tampa Bay Lightning\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Toronto Maple Leafs  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "===========================================\n",
      "[7/8]  Buffalo Sabres  @  New Jersey Devils\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Buffalo Sabres  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "======================================\n",
      "[8/8]  Seattle Kraken  @  Dallas Stars\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  (AWAY)  Seattle Kraken  American odds:  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  skipped game\n",
      "\n",
      "\n",
      "✅ Saved 0 matchup(s) with odds to odds.csv\n",
      "\n",
      "Saved rows preview:\n",
      "Empty DataFrame\n",
      "Columns: [Date, Away Team, Away American Odds, Away Odds, Home Team, Home American Odds, Home Odds]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "DATE_OFFSET = +0\n",
    "TARGET_DATE = (datetime.now() + timedelta(days=DATE_OFFSET)).date()\n",
    "print(f\"📅 Target slate date set to: {TARGET_DATE}\")\n",
    "\n",
    "CSV_PATH = \"odds.csv\"\n",
    "ONLY_PLAYED_GAMES = True  # kept for compatibility (not used elsewhere yet)\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(c))\n",
    "\n",
    "def _american_to_decimal(a):\n",
    "    try:\n",
    "        a = int(a)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if a == 0 or abs(a) < 100 or a == -100:\n",
    "        return None\n",
    "    return 1.0 + (a / 100.0) if a > 0 else 1.0 + (100.0 / abs(a))\n",
    "\n",
    "def _parse_odds(s: str):\n",
    "    if s is None:\n",
    "        return (None, None, None)\n",
    "    s = s.strip().lower().replace(\" \", \"\")\n",
    "    if s in {\"s\", \"skip\"}:\n",
    "        return (None, None, \"skip\")\n",
    "    if s in {\"q\", \"quit\"}:\n",
    "        return (None, None, \"quit\")\n",
    "\n",
    "    # accept decimal\n",
    "    if \".\" in s or \",\" in s:\n",
    "        try:\n",
    "            dec = float(s.replace(\",\", \".\"))\n",
    "            return (None, dec if dec > 1.01 else None, None)\n",
    "        except Exception:\n",
    "            return (None, None, None)\n",
    "\n",
    "    m = re.fullmatch(r\"([+-]?)(\\d{2,4})\", s)\n",
    "    if not m:\n",
    "        return (None, None, None)\n",
    "\n",
    "    sign, num = m.groups()\n",
    "    american = -int(num) if sign == \"-\" else +int(num)\n",
    "    dec = _american_to_decimal(american)\n",
    "    return (american, dec, None) if dec is not None else (None, None, None)\n",
    "\n",
    "def _mk_template(df_games, target_date):\n",
    "    g = df_games.copy()\n",
    "    g[\"Date\"] = pd.to_datetime(g[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    # For past dates -> keep only played games; for future dates -> no filter\n",
    "    today = datetime.now().date()\n",
    "    if target_date < today and {\"Home Score\", \"Away Score\"}.issubset(g.columns):\n",
    "        g = g[g[\"Home Score\"].notna() & g[\"Away Score\"].notna()]\n",
    "\n",
    "    g = g[g[\"Date\"].dt.date == target_date]\n",
    "\n",
    "    out = (\n",
    "        g.drop_duplicates(subset=[\"Date\", \"Home Team\", \"Away Team\"])\n",
    "         .loc[:, [\"Date\", \"Away Team\", \"Home Team\"]]\n",
    "         .assign(**{\n",
    "             \"Away American Odds\": None, \"Away Odds\": None,\n",
    "             \"Home American Odds\": None, \"Home Odds\": None\n",
    "         })\n",
    "         .loc[:, [\n",
    "             \"Date\", \"Away Team\", \"Away American Odds\", \"Away Odds\",\n",
    "             \"Home Team\", \"Home American Odds\", \"Home Odds\"\n",
    "         ]]\n",
    "         .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    out[\"Away Team\"] = out[\"Away Team\"].map(_strip_accents)\n",
    "    out[\"Home Team\"] = out[\"Home Team\"].map(_strip_accents)\n",
    "    return out\n",
    "\n",
    "def _slate_counts_nearby(df_games, center_date, span=3):\n",
    "    g = df_games.copy()\n",
    "    g[\"Date\"] = pd.to_datetime(g[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    counts = []\n",
    "    for delta in range(-1, span + 1):\n",
    "        d = center_date + timedelta(days=delta)\n",
    "        counts.append((d, int((g[\"Date\"].dt.date == d).sum())))\n",
    "    return counts\n",
    "\n",
    "def enter_american_odds(df_games, date_str=None):\n",
    "    target = pd.to_datetime(date_str).date() if date_str else datetime.now().date()\n",
    "    tmpl = _mk_template(df_games, target)\n",
    "\n",
    "    print(f\"\\nTarget date: {target} | games found: {len(tmpl)}\")\n",
    "    print(\"Nearby slates (games per day):\")\n",
    "    for d, n in _slate_counts_nearby(df_games, target, span=3):\n",
    "        mark = \"  (today)\" if d == target else \"\"\n",
    "        print(f\"  {d}: {n}{mark}\")\n",
    "\n",
    "    if tmpl.empty:\n",
    "        print(\"No games for this date in df_all. Pick another date (e.g., enter_american_odds(df_all, '2025-10-12')).\")\n",
    "        return tmpl\n",
    "\n",
    "    print(\"\\nEnter AMERICAN odds (e.g., -120, +135).\")\n",
    "    print(\"Press Enter to leave a side blank, 's' to skip a game, 'q' to quit.\\n\")\n",
    "\n",
    "    def _ask_side(label, team):\n",
    "        while True:\n",
    "            s = input(f\"  ({label})  {team}  American odds: \").strip()\n",
    "            if s == \"\":\n",
    "                return (None, None, None)\n",
    "            a, dec, cmd = _parse_odds(s)\n",
    "            if cmd in {\"quit\", \"skip\"}:\n",
    "                return (None, None, cmd)\n",
    "            if dec is not None:\n",
    "                return (a, dec, None)\n",
    "            print(\"    -> invalid (try -120, +135, or 1.95)\")\n",
    "\n",
    "    for i in range(len(tmpl)):\n",
    "        away = tmpl.at[i, \"Away Team\"]\n",
    "        home = tmpl.at[i, \"Home Team\"]\n",
    "        header = f\"[{i+1}/{len(tmpl)}]  {away}  @  {home}\"\n",
    "        print(\"=\" * len(header))\n",
    "        print(header)\n",
    "        print(\"=\" * len(header))\n",
    "\n",
    "        a_american, a_decimal, cmd = _ask_side(\"AWAY\", away)\n",
    "        if cmd == \"quit\":\n",
    "            break\n",
    "        if cmd == \"skip\":\n",
    "            print(\"  skipped game\\n\")\n",
    "            continue\n",
    "\n",
    "        h_american, h_decimal, cmd2 = _ask_side(\"HOME\", home)\n",
    "        if cmd2 == \"quit\":\n",
    "            break\n",
    "        if cmd2 == \"skip\":\n",
    "            print(\"  skipped game\\n\")\n",
    "            continue\n",
    "\n",
    "        tmpl.at[i, \"Away American Odds\"] = a_american\n",
    "        tmpl.at[i, \"Away Odds\"]          = a_decimal\n",
    "        tmpl.at[i, \"Home American Odds\"] = h_american\n",
    "        tmpl.at[i, \"Home Odds\"]          = h_decimal\n",
    "        print()\n",
    "\n",
    "    complete = tmpl.dropna(subset=[\"Away Odds\", \"Home Odds\"]).reset_index(drop=True)\n",
    "    complete.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"\\n✅ Saved {len(complete)} matchup(s) with odds to {CSV_PATH}\")\n",
    "    return complete\n",
    "\n",
    "# ===== RUN =====\n",
    "df_odds = enter_american_odds(df_games_master, TARGET_DATE.strftime(\"%Y-%m-%d\"))\n",
    "print(\"\\nSaved rows preview:\")\n",
    "print(df_odds.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0bf5bd6-b5fb-4b9e-afb2-28bce1bfdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATE_COL = \"Date\"\n",
    "\n",
    "def _assert_sorted(df, date_col):\n",
    "    if not df[date_col].is_monotonic_increasing:\n",
    "        df.sort_values(date_col, inplace=True, kind=\"mergesort\")\n",
    "\n",
    "def _latest_row_team(df_hist, team, as_of_dt, date_col=DATE_COL):\n",
    "    m = (\n",
    "        (df_hist[date_col] < as_of_dt) &\n",
    "        ((df_hist[\"Home Team\"] == team) | (df_hist[\"Away Team\"] == team))\n",
    "    )\n",
    "    if not m.any():\n",
    "        raise ValueError(f\"No history for team={team} before {as_of_dt.date()}\")\n",
    "    return df_hist.loc[m].iloc[-1]\n",
    "\n",
    "def _latest_value_team(df_hist, team, col_home, col_away, as_of_dt, date_col=DATE_COL):\n",
    "    row = _latest_row_team(df_hist, team, as_of_dt, date_col)\n",
    "    return row[col_home] if row[\"Home Team\"] == team else row[col_away]\n",
    "\n",
    "def build_team_state_asof(df_final, as_of_dt, date_col=DATE_COL):\n",
    "    df_hist = (\n",
    "        df_final.copy()\n",
    "        .assign(**{date_col: pd.to_datetime(df_final[date_col], errors=\"coerce\")})\n",
    "        .dropna(subset=[date_col])\n",
    "        .sort_values(date_col)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    _assert_sorted(df_hist, date_col)\n",
    "\n",
    "    teams = pd.unique(pd.concat([df_hist[\"Home Team\"], df_hist[\"Away Team\"]]).astype(str))\n",
    "\n",
    "    # Precompute rolling column pairs once (cuts a lot of repetition)\n",
    "    rolling_pairs = []\n",
    "    for col in df_hist.columns:\n",
    "        if col.startswith(\"Home_\") and \"_L\" in col:\n",
    "            base = col.replace(\"Home_\", \"\", 1)\n",
    "            acol = \"Away_\" + base\n",
    "            if acol in df_hist.columns:\n",
    "                rolling_pairs.append((base, col, acol))  # (base_name, home_col, away_col)\n",
    "\n",
    "    out = []\n",
    "    for t in map(lambda x: str(x).strip(), teams):\n",
    "        last = _latest_row_team(df_hist, t, as_of_dt, date_col)\n",
    "        rest_days = (as_of_dt.normalize() - pd.to_datetime(last[date_col]).normalize()).days\n",
    "\n",
    "        row = {\"Team\": t, \"RestDays\": rest_days, \"B2B\": int(rest_days == 1)}\n",
    "\n",
    "        # Team-level features (use latest value based on whether team was home/away)\n",
    "        pairs = [\n",
    "            (\"Last 10 Wins\", \"Home Last 10 Wins\", \"Away Last 10 Wins\"),\n",
    "            (\"Win Rate\", \"Home Win Rate\", \"Away Win Rate\"),\n",
    "            (\"Win Streak\", \"Home Team Overall Win Streak Before Game\", \"Away Team Overall Win Streak Before Game\"),\n",
    "            (\"Opponent Strength\", \"Home Opponent Strength\", \"Away Opponent Strength\"),\n",
    "            (\"SRS\", \"Home_SRS\", \"Away_SRS\"),\n",
    "            (\"Rank\", \"Home_Rank\", \"Away_Rank\"),\n",
    "        ]\n",
    "        for out_name, hcol, acol in pairs:\n",
    "            if {hcol, acol}.issubset(df_hist.columns):\n",
    "                row[out_name] = _latest_value_team(df_hist, t, hcol, acol, as_of_dt, date_col)\n",
    "\n",
    "        # Snapshot metrics\n",
    "        for m in [\"PP%\", \"PK%\", \"SV%\", \"xGF%\"]:\n",
    "            hcol, acol = f\"Home_{m}\", f\"Away_{m}\"\n",
    "            if {hcol, acol}.issubset(df_hist.columns):\n",
    "                row[m] = _latest_value_team(df_hist, t, hcol, acol, as_of_dt, date_col)\n",
    "\n",
    "        # Rolling metrics (base stored without Home_/Away_)\n",
    "        for base, hcol, acol in rolling_pairs:\n",
    "            row[base] = _latest_value_team(df_hist, t, hcol, acol, as_of_dt, date_col)\n",
    "\n",
    "        out.append(row)\n",
    "\n",
    "    return pd.DataFrame(out).set_index(\"Team\").sort_index()\n",
    "\n",
    "def build_X_slate(df_odds, df_final, feature_cols, as_of_dt, date_col=DATE_COL):\n",
    "    df_od = df_odds.copy()\n",
    "    df_od[\"Home Team\"] = df_od[\"Home Team\"].astype(str).str.strip()\n",
    "    df_od[\"Away Team\"] = df_od[\"Away Team\"].astype(str).str.strip()\n",
    "\n",
    "    team_state = build_team_state_asof(df_final, as_of_dt, date_col)\n",
    "    X_rows = []\n",
    "\n",
    "    for _, r in df_od.iterrows():\n",
    "        h, a = r[\"Home Team\"], r[\"Away Team\"]\n",
    "        if h not in team_state.index or a not in team_state.index:\n",
    "            raise ValueError(f\"Missing team in history state: {h} or {a}\")\n",
    "\n",
    "        hs, as_ = team_state.loc[h], team_state.loc[a]\n",
    "        feats = {}\n",
    "\n",
    "        # Simple diffs\n",
    "        if \"Rest_Diff\" in feature_cols:\n",
    "            feats[\"Rest_Diff\"] = float(hs[\"RestDays\"]) - float(as_[\"RestDays\"])\n",
    "        if \"B2B_Diff\" in feature_cols:\n",
    "            feats[\"B2B_Diff\"] = int(hs[\"B2B\"]) - int(as_[\"B2B\"])\n",
    "        if \"Home Advantage\" in feature_cols:\n",
    "            feats[\"Home Advantage\"] = 1.0\n",
    "\n",
    "        # Diff features derived from team_state\n",
    "        diff_map = {\n",
    "            \"Last 10 Wins\": (\"Last 10 Wins\", \"Last 10 Wins\"),\n",
    "            \"Win Streak Impact\": (\"Win Streak\", \"Win Streak\"),\n",
    "            \"SRS_Diff\": (\"SRS\", \"SRS\"),\n",
    "            \"Rank Difference\": (\"Rank\", \"Rank\"),\n",
    "        }\n",
    "        for feat_name, (hkey, akey) in diff_map.items():\n",
    "            if feat_name in feature_cols:\n",
    "                feats[feat_name] = float(hs.get(hkey, np.nan)) - float(as_.get(akey, np.nan))\n",
    "\n",
    "        # Opp strength (kept as two separate features)\n",
    "        if \"Home Opponent Strength\" in feature_cols:\n",
    "            feats[\"Home Opponent Strength\"] = float(hs.get(\"Opponent Strength\", np.nan))\n",
    "        if \"Away Opponent Strength\" in feature_cols:\n",
    "            feats[\"Away Opponent Strength\"] = float(as_.get(\"Opponent Strength\", np.nan))\n",
    "\n",
    "        # Rolling deltas\n",
    "        for c in feature_cols:\n",
    "            if c.startswith(\"Delta_\") and \"_L\" in c:\n",
    "                metric = c.replace(\"Delta_\", \"\", 1)  # ex: \"PP%_L5\"\n",
    "                feats[c] = float(hs.get(metric, np.nan)) - float(as_.get(metric, np.nan))\n",
    "\n",
    "        row = {fc: feats.get(fc, np.nan) for fc in feature_cols}\n",
    "        row[\"Home_Team\"] = h\n",
    "        row[\"Away_Team\"] = a\n",
    "        X_rows.append(row)\n",
    "\n",
    "    X_raw = pd.DataFrame(X_rows)\n",
    "    rename_map = {c: c.replace(\" \", \"_\") for c in X_raw.columns}\n",
    "    X = X_raw.rename(columns=rename_map)\n",
    "    return X, rename_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c88034-7149-4649-b9b9-d43c1e4ab3f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No rows with both Home Odds and Away Odds. Nothing to predict.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m df_out \u001b[38;5;241m=\u001b[39m df_out\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHome Odds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAway Odds\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_out\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo rows with both Home Odds and Away Odds. Nothing to predict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ---- build X slate (aligned to training meta) ----\u001b[39;00m\n\u001b[0;32m     39\u001b[0m X_slate, rename_map_live \u001b[38;5;241m=\u001b[39m build_X_slate(\n\u001b[0;32m     40\u001b[0m     df_odds\u001b[38;5;241m=\u001b[39mdf_out,\n\u001b[0;32m     41\u001b[0m     df_final\u001b[38;5;241m=\u001b[39mdf_final,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     date_col\u001b[38;5;241m=\u001b[39mDATE_COL_IN_HISTORY,\n\u001b[0;32m     45\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: No rows with both Home Odds and Away Odds. Nothing to predict."
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"model_LOGISTIC_PHASE1_FINAL.joblib\"\n",
    "MODEL_META_PATH = \"model_LOGISTIC_PHASE1_FINAL_meta.joblib\"\n",
    "\n",
    "BANKROLL = 200\n",
    "PREDICTIONS_CSV = \"predictions.csv\"\n",
    "DATE_OFFSET = 0\n",
    "DATE_COL_IN_HISTORY = \"Date\"\n",
    "\n",
    "TARGET_DATE = (datetime.now() + timedelta(days=DATE_OFFSET)).date()\n",
    "as_of_dt = pd.Timestamp(TARGET_DATE)\n",
    "\n",
    "# ---- required objects ----\n",
    "for name in (\"df_odds\", \"df_final\"):\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Missing `{name}` in memory.\")\n",
    "\n",
    "for path in (MODEL_PATH, MODEL_META_PATH):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "meta = joblib.load(MODEL_META_PATH)\n",
    "\n",
    "feature_cols = meta[\"feature_cols\"]\n",
    "use_team_dummies = meta[\"use_team_dummies\"]\n",
    "\n",
    "# ---- prep odds first (avoid rebuilding X twice) ----\n",
    "df_out = df_odds.copy()\n",
    "df_out[\"SlateDate\"] = TARGET_DATE.isoformat()\n",
    "\n",
    "for c in (\"Home Odds\", \"Away Odds\"):\n",
    "    df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\")\n",
    "\n",
    "df_out = df_out.dropna(subset=[\"Home Odds\", \"Away Odds\"]).reset_index(drop=True)\n",
    "if df_out.empty:\n",
    "    raise ValueError(\"No rows with both Home Odds and Away Odds. Nothing to predict.\")\n",
    "\n",
    "# ---- build X slate (aligned to training meta) ----\n",
    "X_slate, rename_map_live = build_X_slate(\n",
    "    df_odds=df_out,\n",
    "    df_final=df_final,\n",
    "    feature_cols=feature_cols,\n",
    "    as_of_dt=as_of_dt,\n",
    "    date_col=DATE_COL_IN_HISTORY,\n",
    ")\n",
    "\n",
    "# ---- validate required columns exist ----\n",
    "expected = [c.replace(\" \", \"_\") for c in feature_cols]\n",
    "if use_team_dummies:\n",
    "    expected += [\"Home_Team\", \"Away_Team\"]\n",
    "\n",
    "missing = [c for c in expected if c not in X_slate.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns required for inference: {missing}\")\n",
    "\n",
    "# ---- predict ----\n",
    "p_home = np.clip(model.predict_proba(X_slate)[:, 1], 1e-6, 1 - 1e-6)\n",
    "df_out[\"p_home\"] = np.round(p_home, 4)\n",
    "\n",
    "# ---- Kelly sizing ----\n",
    "def _kelly(p_win: float, dec_odds: float) -> float:\n",
    "    b = dec_odds - 1.0\n",
    "    if b <= 0:\n",
    "        return 0.0\n",
    "    q = 1.0 - p_win\n",
    "    return max(0.0, (b * p_win - q) / b)\n",
    "\n",
    "home_pre = np.array([_kelly(p, o) for p, o in zip(p_home, df_out[\"Home Odds\"])], dtype=float) * BANKROLL\n",
    "away_pre = np.array([_kelly(1.0 - p, o) for p, o in zip(p_home, df_out[\"Away Odds\"])], dtype=float) * BANKROLL\n",
    "\n",
    "total_pre = float(np.nansum(home_pre) + np.nansum(away_pre))\n",
    "scaled = BANKROLL > 0 and total_pre > BANKROLL\n",
    "\n",
    "if scaled:\n",
    "    s = BANKROLL / total_pre\n",
    "    home_bet = np.round(home_pre * s).astype(int)\n",
    "    away_bet = np.round(away_pre * s).astype(int)\n",
    "else:\n",
    "    home_bet = np.round(home_pre).astype(int)\n",
    "    away_bet = np.round(away_pre).astype(int)\n",
    "\n",
    "df_out[\"Scaled\"] = scaled\n",
    "df_out[\"Home Bet\"] = np.where(home_bet > 0, home_bet, 0)\n",
    "df_out[\"Away Bet\"] = np.where(away_bet > 0, away_bet, 0)\n",
    "\n",
    "cols = [\"SlateDate\", \"Away Team\", \"Away Bet\", \"Away Odds\", \"Home Team\", \"Home Bet\", \"Home Odds\", \"p_home\", \"Scaled\"]\n",
    "print(df_out[cols])\n",
    "\n",
    "# ---- save append ----\n",
    "write_header = not os.path.exists(PREDICTIONS_CSV)\n",
    "mode = \"w\" if write_header else \"a\"\n",
    "df_out[cols].to_csv(PREDICTIONS_CSV, index=False, mode=mode, header=write_header)\n",
    "\n",
    "print(f\"\\nSaved {len(df_out)} rows to {PREDICTIONS_CSV} (mode='{mode}', header={write_header})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bdca5-2c5b-42e1-b5eb-c1806ae625da",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_final.to_csv(\"df_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b96dc-d233-4f0e-8f66-8a0422568eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
